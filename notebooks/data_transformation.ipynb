{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18bad3d",
   "metadata": {},
   "source": [
    "## 2. Initial Data Transformation\n",
    "\n",
    "## Task description\n",
    "Join the equivalent of the contents of the file city-hex-polygons-8.geojson to the service request dataset, such that each service request is assigned to a single H3 resolution level 8 hexagon. Use the sr_hex.csv.gz file to validate your work.\n",
    "\n",
    "For any requests where the Latitude and Longitude fields are empty, set the index value to 0. Use your judgement to include any other appropriate validation.\n",
    "\n",
    "Include logging that lets the executor know how many of the records failed to join, and include a join error threshold above which the script will error out. Please motivate why you have selected the error threshold that you have. Please also log the time taken to perform the operations described, and within reason, try to optimise latency and computational resources used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f8852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a584fcc",
   "metadata": {},
   "source": [
    "Failure rate: the `FAILURE_THRESHOLD` is set to 5%. Without more content, this is a reasonable tolerance because:\n",
    "\n",
    "- It allows for minor coordinate issues (e.g. bad GPS data, out-of-bounds requests).\n",
    "- But it will catch systemic issues (e.g. wrong CRS or missing polygons).\n",
    "\n",
    "In practice, a reasonable tolerance will be set taking into account the specific requirements of the request (e.g. the level of accuracy required and the 'cost' of an error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc9d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"../data/sr.csv.gz\"\n",
    "HEX_POLYGONS_PATH = \"../data/city-hex-polygons-8.geojson\"\n",
    "TEST_DATA_PATH = \"../data/sr_hex.csv.gz\"\n",
    "\n",
    "# Failure threshold (5%)\n",
    "FAILURE_THRESHOLD = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8fb219",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read service requests data\n",
    "sr = pd.read_csv(INPUT_PATH)\n",
    "sr_hex = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "# Read geospatial data\n",
    "poly8 = gpd.read_file(HEX_POLYGONS_PATH)\n",
    "poly8 = poly8.rename(columns={\"index\": \"h3_level8_index\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b589c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for uniqueness of notification_number in both datasets\n",
    "assert sr[\"notification_number\"].is_unique\n",
    "assert sr_hex[\"notification_number\"].is_unique\n",
    "assert set(sr[\"notification_number\"]) == set(sr_hex[\"notification_number\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d0626",
   "metadata": {},
   "source": [
    "The next step performs the geospatial join. To do the spatial join, we use `sjoin_nearest` from GeoPandas. For each service request, the `sjoin_nearest` function joins it to the nearest hexagon, based on the centroid of that hexagon.\n",
    "\n",
    "This is the most time-consuming step, but should not take longer than 8 minutes, even when benchmarked on older hardware. I did consider using the H3 Hexagonal Grid System along with Numpy for distance calculations to optimise for speed, but ultimately concluded that the potential speed gain was not worth the additional complexity for a dataset of this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214c2d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  notification_number  reference_number  \\\n",
      "0           0            400583534      9.109492e+09   \n",
      "1           1            400555043      9.108995e+09   \n",
      "2           2            400589145      9.109614e+09   \n",
      "3           3            400538915      9.108601e+09   \n",
      "4           4            400568554               NaN   \n",
      "\n",
      "          creation_timestamp       completion_timestamp     directorate  \\\n",
      "0  2020-10-07 06:55:18+02:00  2020-10-08 15:36:35+02:00  URBAN MOBILITY   \n",
      "1  2020-07-09 16:08:13+02:00  2020-07-14 14:27:01+02:00  URBAN MOBILITY   \n",
      "2  2020-10-27 10:21:59+02:00  2020-10-28 17:48:15+02:00  URBAN MOBILITY   \n",
      "3  2020-03-19 06:36:06+02:00  2021-03-29 20:34:19+02:00  URBAN MOBILITY   \n",
      "4  2020-08-25 09:48:42+02:00  2020-08-31 08:41:13+02:00  URBAN MOBILITY   \n",
      "\n",
      "                        department            branch  \\\n",
      "0  Roads Infrastructure Management  RIM Area Central   \n",
      "1  Roads Infrastructure Management     RIM Area East   \n",
      "2  Roads Infrastructure Management     RIM Area East   \n",
      "3  Roads Infrastructure Management    RIM Area North   \n",
      "4  Roads Infrastructure Management    RIM Area South   \n",
      "\n",
      "                    section                    code_group  \\\n",
      "0      District: Blaauwberg  TD Customer complaint groups   \n",
      "1  District : Somerset West  TD Customer complaint groups   \n",
      "2  District : Somerset West  TD Customer complaint groups   \n",
      "3      District : Bellville  TD Customer complaint groups   \n",
      "4        District : Athlone  TD Customer complaint groups   \n",
      "\n",
      "                                     code cause_code_group         cause_code  \\\n",
      "0  Pothole&Defect Road Foot Bic Way/Kerbs       Road (RCL)      Wear and tear   \n",
      "1                Manhole Cover/Gully Grid       Road (RCL)          Vandalism   \n",
      "2                Manhole Cover/Gully Grid       Road (RCL)          Vandalism   \n",
      "3              Paint Markings Lines&Signs    Road Markings      Wear and tear   \n",
      "4  Pothole&Defect Road Foot Bic Way/Kerbs       Road (RCL)  Surfacing failure   \n",
      "\n",
      "    official_suburb   latitude  longitude                        geometry  \\\n",
      "0  MONTAGUE GARDENS -33.872839  18.522488  POINT (270839.518 6249180.748)   \n",
      "1     SOMERSET WEST -34.078916  18.848940  POINT (301520.676 6227005.661)   \n",
      "2            STRAND -34.102242  18.821116  POINT (299008.038 6224364.226)   \n",
      "3        RAVENSMEAD -33.920019  18.607209  POINT (278799.651 6244133.664)   \n",
      "4         CLAREMONT -33.987400  18.453760  POINT (264796.291 6236318.357)   \n",
      "\n",
      "   h3_level8_index  \n",
      "0  88ad360225fffff  \n",
      "1  88ad36d5e1fffff  \n",
      "2  88ad36d437fffff  \n",
      "3  88ad361133fffff  \n",
      "4  88ad361709fffff  \n",
      "CPU times: user 7min 27s, sys: 670 ms, total: 7min 27s\n",
      "Wall time: 7min 35s\n"
     ]
    }
   ],
   "source": [
    "df = gpd.GeoDataFrame(\n",
    "    sr,\n",
    "    geometry=gpd.points_from_xy(sr[\"longitude\"], sr[\"latitude\"]),\n",
    "    crs=\"EPSG:4326\"   # WGS84 latitude/longitude\n",
    ")\n",
    "df = df.to_crs(\"EPSG:32734\")  # Convert to UTM Zone 34S\n",
    "\n",
    "# Ensure both GeoDataFrames have the same coordinate reference system\n",
    "poly8 = poly8.to_crs(df.crs)\n",
    "\n",
    "# Perform the spatial join\n",
    "# df = gpd.sjoin(df, poly8[[\"h3_level8_index\", \"geometry\"]], how=\"left\")\n",
    "df = gpd.sjoin_nearest(df, poly8[[\"h3_level8_index\", \"geometry\"]], how=\"left\")\n",
    "df = df.drop(columns=[\"index_right\"])\n",
    "# Set `h3_level8_index` to '0' where coordinates are missing\n",
    "df.loc[df[\"longitude\"].isna() | df[\"latitude\"].isna(), \"h3_level8_index\"] = '0'\n",
    "\n",
    "# Record the number of records that failed to join (excluding those with missing coordinates)\n",
    "num_failed_joins = df[\"h3_level8_index\"].isna().sum()\n",
    "# Calculate the failure rate (excluding those with missing coordinates)\n",
    "num_invalid_coords = (df[\"longitude\"].isna() | df[\"latitude\"].isna()).sum()\n",
    "num_valid_coords = df.shape[0] - num_invalid_coords\n",
    "failure_rate = num_failed_joins / num_valid_coords\n",
    "logging.info(f\"Total records: {df.shape[0]}\")\n",
    "logging.info(f\"Records with missing coordinates: {num_invalid_coords}\")\n",
    "logging.info(f\"Failed joins: {num_failed_joins} ({failure_rate*100:.2f}%)\")\n",
    "\n",
    "# Script errors out if failure rate exceeds threshold\n",
    "if failure_rate > FAILURE_THRESHOLD:\n",
    "    logging.error(\"Join failure rate exceeds threshold (%.2f%%)\", failure_rate * 100)\n",
    "    sys.exit(1)\n",
    "\n",
    "# End timing\n",
    "elapsed = time.perf_counter() - start_time\n",
    "logging.info(f\"Join completed in {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1722139a",
   "metadata": {},
   "source": [
    "Compare with `sr_hex`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b458f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with `sr_hex`\n",
    "df_test = (\n",
    "    df[[\"notification_number\", \"h3_level8_index\", \"longitude\", \"latitude\"]]\n",
    "    .merge(\n",
    "        sr_hex[[\"notification_number\", \"h3_level8_index\", \"longitude\", \"latitude\"]],\n",
    "        on=\"notification_number\", how=\"left\", suffixes=('_calc', '_test')\n",
    "    )\n",
    ")\n",
    "df_test[\"match\"] = df_test[\"h3_level8_index_calc\"] == df_test[\"h3_level8_index_test\"]\n",
    "print(\"Mismatched records compared to `sr_hex.csv`:\")\n",
    "print(df_test['match'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f3aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notification_number</th>\n",
       "      <th>h3_level8_index_calc</th>\n",
       "      <th>longitude_calc</th>\n",
       "      <th>latitude_calc</th>\n",
       "      <th>h3_level8_index_test</th>\n",
       "      <th>longitude_test</th>\n",
       "      <th>latitude_test</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>462434</th>\n",
       "      <td>1015950287</td>\n",
       "      <td>88ad36c605fffff</td>\n",
       "      <td>18.774378</td>\n",
       "      <td>-34.044257</td>\n",
       "      <td>88ad36c629fffff</td>\n",
       "      <td>18.774378</td>\n",
       "      <td>-34.044257</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804983</th>\n",
       "      <td>1016348681</td>\n",
       "      <td>88ad36c605fffff</td>\n",
       "      <td>18.774378</td>\n",
       "      <td>-34.044257</td>\n",
       "      <td>88ad36c629fffff</td>\n",
       "      <td>18.774378</td>\n",
       "      <td>-34.044257</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924595</th>\n",
       "      <td>1016488950</td>\n",
       "      <td>88ad361b5bfffff</td>\n",
       "      <td>18.723060</td>\n",
       "      <td>-33.904955</td>\n",
       "      <td>88ad361b51fffff</td>\n",
       "      <td>18.723060</td>\n",
       "      <td>-33.904955</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        notification_number h3_level8_index_calc  longitude_calc  \\\n",
       "462434           1015950287      88ad36c605fffff       18.774378   \n",
       "804983           1016348681      88ad36c605fffff       18.774378   \n",
       "924595           1016488950      88ad361b5bfffff       18.723060   \n",
       "\n",
       "        latitude_calc h3_level8_index_test  longitude_test  latitude_test  \\\n",
       "462434     -34.044257      88ad36c629fffff       18.774378     -34.044257   \n",
       "804983     -34.044257      88ad36c629fffff       18.774378     -34.044257   \n",
       "924595     -33.904955      88ad361b51fffff       18.723060     -33.904955   \n",
       "\n",
       "        match  \n",
       "462434  False  \n",
       "804983  False  \n",
       "924595  False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[~df_test['match']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_code_challenge (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
